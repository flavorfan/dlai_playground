{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Message Passing Interface MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction  to MPI\n",
    "The Message Passing Interface (MPI) is:\n",
    "\n",
    "- Particularly useful for - but not limited to - distributed memory machines.\n",
    "- The _de facto_ standard parallel programming interface.\n",
    "\n",
    "Many implementations exist - MPICH, OpenMPI, ...\n",
    "\n",
    "Interfaces in: \n",
    "- C/C++ \n",
    "- Fortran and ... \n",
    "- Python wrappers (mpi4py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is message passing?\n",
    "The program is launched as separate processes (tasks) each with their own address space.\n",
    "- To achieve parallelism we could have each process work on different data \n",
    "\n",
    "Information must be **explicitly moved** from process to process:\n",
    "- A task can access the data of another process through passing messages (a copy of the data is passed from one process to another)\n",
    "\n",
    "Two main classes of message passing:\n",
    "- **Point-to-point** operations, involving only two processes\n",
    "- **Collective** operations, involving a group of processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## mpi4py\n",
    "\n",
    "mpi4py supports both point-to-point and collective communications for \"generic\" Python objects as well as \"buffer-type\" objects\n",
    "- for generic Python objects, use the all-lowercase methods, e.g. send, recv, isend, irecv, bcast, scatter, gather, reduce\n",
    "- for buffer-type objects (e.g. numpy arrays) use the uppercase versions, e.g. Send, ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hello World!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile hello_world_mpi.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "print('Hello from process {} out of {}'.format(rank, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communicators\n",
    "A communicator is a group of processes that can talk to each other. \n",
    "\n",
    "- the size of the communicator is obtained by the `Get_size` method\n",
    "\n",
    "Within that group each process is assigned a unique rank (assigned by the system when we launch the program).\n",
    "\n",
    "- The rank of each process is retrieved via the `Get_rank` method \n",
    "\n",
    "In the above code, we defined set \"comm\" to the default communicator `MPI.COMM_WORLD`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing an MPI code\n",
    "\n",
    "There is nothing in the MPI standard that specifies how an MPI code should be executed. \n",
    "\n",
    "Typically, it will be launched with `mpirun` or `mpiexec` followed by python\n",
    "e.g. `mpirun -n 4 python3 hello_world_python.py`\n",
    "\n",
    "On Piz Daint we launch with `srun`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! srun -n 8 python hello_world_mpi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Point-to-point communication\n",
    "\n",
    "Point-to-point communication is sending message/data from one process to another. \n",
    "\n",
    "For point-to-point communication between Python objects, mpi4py provides the `send` and `recv` methods that are similar to those in MPI. \n",
    "\n",
    "In the following example, rank 0 passes a Python dictionary object to another processes (rank 1):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile send_recv.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'a': 9, 'b': 5.001}\n",
    "    comm.send(data, dest=1, tag=42)\n",
    "    print('Process {} sent data:'.format(rank), data)\n",
    "    \n",
    "elif rank == 1:\n",
    "    data = comm.recv(source=0, tag=42)\n",
    "    print('Process {} received data:'.format(rank), data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! srun -n 2 python send_recv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning alert-block alert-info\"><b>Exercise:</b> Generalize the above example so that the master process sends the dictionary to an arbitrary number of processes. Use point-to-point communication.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile send_recv_for_loop.py\n",
    "# write your solution here\n",
    "# hint: add a for loop for the send\n",
    "# hint: use the rank as the tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it out:\n",
    "# if your code hangs/deadlocks, you might need to `killall srun` from the terminal.\n",
    "! srun -n 6 python send_recv_for_loop.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a sample solution\n",
    "%pycat send_recv_for_loop_solution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonblocking point-to-point communication\n",
    "In the above examples, the functions block the caller until the data buffers involved in the communication can be safely reused by the application program.\n",
    "\n",
    "For potentially increased performance we can try to overlap communication and computation by using nonblocking send and receives.\n",
    "\n",
    "`isend` and `irecv` are non-blocking methods that immediately return `Request` objects.\n",
    "\n",
    "We can use the `wait` method to manage the completion of the communication. \n",
    "\n",
    "You can then perform computation between `comm.isend(...)` and `req.wait()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile isend_irecv_for_loop.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'a': 9, 'b': 5.001}\n",
    "    for i in range(1, size):\n",
    "        req = comm.isend(data, dest=i, tag=i)\n",
    "        req.wait()\n",
    "        print('Process {} sent data:'.format(rank), data)\n",
    "\n",
    "else:\n",
    "    req = comm.irecv(source=0, tag=rank)\n",
    "    data = req.wait()\n",
    "    print('Process {} received data:'.format(rank), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! srun -n 12 python isend_irecv_for_loop.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Collective communication \n",
    "Generally groups of processes need to exchange messages between themselves. Rather than explicitly sending and receiving such messages from point to point, MPI comes with group operations known as collectives.\n",
    "- Broadcast, scatter, gather and reduction\n",
    "- Implementations can optimize performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Broadcast\n",
    "Send from one process to all other processes.\n",
    "\n",
    "![broadcast](03-broadcast.png) \n",
    "\n",
    "We can rewrite our above exmaple using a broadcast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile broadcast.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    data = {'a': 9, 'b': 5.001}\n",
    "           \n",
    "else:\n",
    "    data = None\n",
    "data = comm.bcast(data, root=0)\n",
    "print('Process {} received data:'.format(rank), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! srun -n 12 python broadcast.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scatter\n",
    "Split data into chunks and send a chunk to individual processes to work on.\n",
    "\n",
    "![scatter](03-scatter.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scatter.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if rank == 0:\n",
    "    data = [(i+1)**2 for i in range(size)]\n",
    "else:\n",
    "    data = None\n",
    "data = comm.scatter(data, root=0)\n",
    "print('Process {} received data:'.format(rank), data)\n",
    "assert data == (rank+1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! srun -n 4 python scatter.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gather\n",
    "Gather the chunks and bring them to the root process\n",
    "\n",
    "![gather](03-gather.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile gather.py\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "data = (rank+1)**2\n",
    "data = comm.gather(data, root=0)\n",
    "if rank == 0:\n",
    "    for i in range(size):\n",
    "        assert data[i] == (i+1)**2\n",
    "else:\n",
    "    assert data is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! srun -n 4 python gather.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In contrast, allgather will return the results to all processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reduction\n",
    "\n",
    "Gather results, and then do some computation.\n",
    "\n",
    "Examples of reductions are:\n",
    "- MPI_SUM - Sums the elements. \n",
    "- MPI_PROD - Multiplies all elements. \n",
    "- MPI_MAX - Returns the maximum element \n",
    "- MPI_MIN - Returns the minimum element\n",
    "\n",
    "![reduce](03-reduce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile reduction.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "# element assigned to be the rank of the processor\n",
    "value = np.array(rank,'d')\n",
    "\n",
    "print(' Rank: ',rank, ' value = ', value)\n",
    "\n",
    "# initialize the np arrays that will store the results:\n",
    "value_sum      = np.array(0.0,'d')\n",
    "value_max      = np.array(0.0,'d')\n",
    "\n",
    "# perform the reductions:\n",
    "comm.Reduce(value, value_sum, op=MPI.SUM, root=0)\n",
    "comm.Reduce(value, value_max, op=MPI.MAX, root=0)\n",
    "\n",
    "if rank == 0:\n",
    "    print(' Rank 0: value_sum =    ',value_sum)\n",
    "    print(' Rank 0: value_max =    ',value_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! srun -n 10 python reduction.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lower case vs. upper case versions\n",
    "In Python there are two versions of the various MPI methods:\n",
    "\n",
    "- lower case (send, recv, gather, etc.)\n",
    "- Upper case (Send, Recv, Gather, etc.) \n",
    "\n",
    "You can transmit arbitrary Python data types using the lower-case version of the methods. mpi4py will serialize the data type, send it to the remote process, then deserialize it back to the original data type (a process known as \"pickling\" and \"unpickling\"). This can add significant overhead to the MPI operation.\n",
    "\n",
    "For sending \"buffer-like\" objects, you can use the upper-case versions. The data object must support Python's \"single-segment buffer interface\". Examples of objects you can send this way are numpy arrays and strings.\n",
    "\n",
    "Use the upper-case versions where possible as they will be faster (close to the speed of MPI communication in C).\n",
    "\n",
    "- the memory of the receiving buffer needs to be allocated prior to the communication\n",
    "- the size of the sending buffer should not exceed that of the receiving buffer \n",
    "- mpi4py expects the buffer-like objects to have contiguous memory (e.g. as is the case with numpy arrays) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here we pass a numpy array from the master node to the other processes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile uppercase.py\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "size = comm.Get_size()\n",
    "\n",
    "if rank == 0:\n",
    "    data = np.arange(4.)\n",
    "    # master process sends data to worker processes by\n",
    "    # going through the ranks of all worker processes\n",
    "    for i in range(1, size):\n",
    "        comm.Send(data, dest=i, tag=i)\n",
    "        print('Process {} sent data:'.format(rank), data)\n",
    "\n",
    "else:\n",
    "    # initialize the receiving buffer\n",
    "    data = np.zeros(4)\n",
    "    # receive data from master process\n",
    "    comm.Recv(data, source=0, tag=rank)\n",
    "    print('Process {} received data:'.format(rank), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! srun -n 8 python uppercase.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "- the data array has to exist at the time of the `Recv` call.\n",
    "- the `Recv` method takes data as the first argument (in contrast to the `recv` method which returns the data object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- `mpi4py` is the most commonly used Python interface to MPI \n",
    "- MPI calls are via the communicator object\n",
    "- You can communicate arbitrary Python objects\n",
    "- NumPy arrays can be communicated with similar speed to C and Fortran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using mpi4py in a Notebook\n",
    "\n",
    "So far we have been running our MPI code with `srun -n X ... python <code>`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To use MPI directly in a Jupyter Notebook you need to combine:\n",
    "\n",
    "- mpi4py, and\n",
    "- IPython Parallel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## IPython Parallel \n",
    "\n",
    "In regular IPython we have a client (the frontend) and a kernel which executes the code. And they communciate with messages. \n",
    "\n",
    "So, as IPython already does remote execution... if you have _one_ remote kernel, why not have _one hundred_?\n",
    "\n",
    "These are called IPython Parallel \"engines\"\n",
    "\n",
    "<div>\n",
    "<img src=\"ipyparallel.png\" style=\"width:300px\"/>\n",
    "</div>\n",
    "Rather than having clients (blue) connect directly to kernels (green) as in notebook, you have an intermediary of a hub (with schedulers) - known as the \"controller\". The client communicates only with the controller. The controller keeps track of the available engines and forwards requests from the client to the engines. It schedules the work and monitors its status. The results are communicated through the controller back to the client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use IPython for parallel computing, you need to start one instance of the controller and one or more instances of the engines. The controller and each engine can run on different machines or on the same machine.\n",
    "\n",
    "There are three ways to start the controller and engines:\n",
    "\n",
    "- Separately, using the **ipcontroller** and **ipengine** commands.\n",
    "- In an automated manner using the **ipcluster** command.\n",
    "- From a custom **magic** developed inhouse `import ipcmagic`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the first method, which is \"manual\", but provides the most transparency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> The following commands need to be entered in a terminal. File > New > Terminal. A terminal will open as a new tab. Grab the tab and pull it to the right to have the terminal next to your notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ ipcontroller start &\n",
    "$ srun -n 4 ipengine --mpi \n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The IPython Parallel engines need to be started using the `mpirun` command (or equivalent). On our system:\n",
    "\n",
    "- Start the **ipcontroller** \n",
    "- Start the **ipengines** using `srun` and with `--mpi` argument.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we access our \"Cluster\". [IPython][IP] comes with a module [ipyparallel][IPp] that is used to access the engines, we just started. We first need to import Client.\n",
    "\n",
    "[IPp]: https://ipyparallel.readthedocs.io/en/latest/\n",
    "[IP]: http://www.ipython.org\n",
    "\n",
    "The client is started by first importing it from ipyparallel and then by initalizing it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Note:</b> If you receive an error ModuleNotFoundError: No module named 'ipyparallel', ensure that you have the miniconda-pythonhpc kernel loaded\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc = ipp.Client(profile=\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the ids of the engines attached:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc.ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Magics\n",
    "\n",
    "IPython makes it very easy to use IPyParallel. It provides the magic commands ``%px`` and ``%%px`` to execute code in parallel. The target attribute is used to pick the engines, you want. By default, all the engines of the last Client object created are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px --target 0:2\n",
    "import os, socket\n",
    "print(os.getpid())\n",
    "print(socket.gethostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = MPI.COMM_WORLD.Get_rank()\n",
    "size = MPI.COMM_WORLD.Get_size()\n",
    "\n",
    "A = np.zeros((size,size))\n",
    "if rank==0:\n",
    "    A = np.random.randn(size, size)\n",
    "    print(\"Original array on root process\\n\", A)\n",
    "local_a = np.zeros(size)\n",
    "\n",
    "comm.Scatter(A, local_a, root=0)\n",
    "print(\"Process\", rank, \"received\", local_a)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "miniconda-pythonhpc",
   "language": "python",
   "name": "miniconda-pythonhpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
