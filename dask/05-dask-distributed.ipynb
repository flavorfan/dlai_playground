{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask Distributed \n",
    "\n",
    "`Dask.distributed` is a centrally managed, distributed, dynamic task scheduler. The central `dask-scheduler` process coordinates the actions of one or more `dask-worker`s processes that might be spread across multiple machines.\n",
    "\n",
    "In a nutshell, `dask.distributed` extends `dask` to distributed computing on multiple nodes. \n",
    "\n",
    "It can also be used as the scheduler for a single/local node.\n",
    "\n",
    "One way to start `dask.distributed` locally is with the Client interface. If you create a client without providing an address it will start up a local scheduler and worker for you.\n",
    "\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "client\n",
    "```\n",
    "\n",
    "Another way is to start the scheduler and workers by hand. This allows dask.distributed to use multiple machines as workers.\n",
    "\n",
    "```\n",
    "$ dask-scheduler\n",
    "Scheduler started at 127.0.0.1:8786\n",
    "\n",
    "$ dask-worker 127.0.0.1:8786\n",
    "$ dask-worker 127.0.0.1:8786\n",
    "$ dask-worker 127.0.0.1:8786\n",
    "```\n",
    "Here the dask-workers could be started on the same machine or different ones.\n",
    "\n",
    "Launch a Client and point it to the IP/port of the scheduler.\n",
    "\n",
    "```\n",
    "from dask.distributed import Client\n",
    "client = Client('127.0.0.1:8786')\n",
    "```\n",
    "\n",
    "If you are interested in the details of the current scheduler/worker setup issue:\n",
    "```\n",
    "client.scheduler_info()\n",
    "```\n",
    "\n",
    "`dask.distributed` features a sophisticated **web-based monitoring** based on the package `bokeh`. This is exposed in Juptyerlab as a JupyterLab extension. You can use the extension both to start a distributed cluster on your local node, and to examine performance. This is the method we will use to start the scheduler and workers later in this noteobok. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following examples demonstrate the dask dashboard for use with dask.delayed (which can be used to parallelize generic Python code), and dask.array (which extends numpy arrays to larger than memory).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning alert-block alert-info\">\n",
    "<b>Note:</b> Use the default \"Python 3\" kernel for thse exercises, not the miniconda kernel. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delayed example\n",
    "\n",
    "As we've seen Dask.delayed is a simple and powerful way to parallelize existing code.  It allows you to delay function calls into a task graph with dependencies.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Python code\n",
    "\n",
    "Our example is similar to what we've seen earlier - we simulate work using the sleep function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def inc(x):\n",
    "    from time import sleep\n",
    "    sleep(2)\n",
    "    return x + 1\n",
    "\n",
    "def dec(x):\n",
    "    from time import sleep\n",
    "    sleep(2)\n",
    "    return x - 1\n",
    "    \n",
    "def add(x, y):\n",
    "    from time import sleep\n",
    "    sleep(0.5)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sequentially\n",
    "\n",
    "Should take 4.5 seconds..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = inc(1)\n",
    "y = dec(2)\n",
    "z = add(x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate the Python functions with dask\n",
    "\n",
    "\n",
    "These now become lazy versions. Rather than computing the result immediately, they record what we want to compute and stick that task into a graph that we'll run later in parallel using the distributed scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "inc = dask.delayed(inc)\n",
    "dec = dask.delayed(dec)\n",
    "add = dask.delayed(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling these lazy functions takes no time, but we are only constructing a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = inc(1)\n",
    "y = dec(2)\n",
    "z = add(x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize(rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute with threads on our local machine\n",
    "Should take three seconds, as we can do the inc and dec in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, create a distributed cluster \n",
    "\n",
    "We connect to our cluster. Now rather than running locally, all of our computations will happen on our cluster. In this example, we create a distributed cluster on the node that we are already running on. But the distributed cluster could be a remote HPC system, an Amazon instance, etc.\n",
    "\n",
    "Create a cluster from the Dask dashboard tab on the left hand side. To create a cluster click `+NEW`. Drag over the client code into this notebook and execute it. The code cell will look similar to the following:\n",
    "\n",
    "```\n",
    "from dask.distributed import Client\n",
    "client = Client(\"tcp://127.0.0.1:44443\")\n",
    "client\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrange the dask task stream, dask progress, and/or other components in your workspace and monitor performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.scheduler_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelize normal Python code\n",
    "\n",
    "Now we use Dask in \"for loop\" Python code. This generates graphs instead of doing computations directly, but still looks like the code we had before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "zs = []\n",
    "for i in range(256):\n",
    "    x = inc(i)\n",
    "    y = dec(x)\n",
    "    z = add(x, y)\n",
    "    zs.append(z)\n",
    "    \n",
    "zs = dask.persist(*zs)\n",
    "total = dask.delayed(sum)(zs)\n",
    "total.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the Dask dashboard we can see that Dask spreads this work around our cluster, managing load balancing, dependencies, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom computation: a tree summation\n",
    "\n",
    "As an example of a non-trivial algorithm, consider the classic tree reduction.  We accomplish this with a nested for loop and a bit of normal Python logic.\n",
    "\n",
    "```\n",
    "finish           total             single output\n",
    "    ^          /        \\\n",
    "    |        c1          c2        neighbors merge\n",
    "    |       /  \\        /  \\\n",
    "    |     b1    b2    b3    b4     neighbors merge\n",
    "    ^    / \\   / \\   / \\   / \\\n",
    "start   a1 a2 a3 a4 a5 a6 a7 a8    many inputs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = zs\n",
    "while len(L) > 1:\n",
    "    new_L = []\n",
    "    for i in range(0, len(L), 2):\n",
    "        lazy = add(L[i], L[i + 1])  # add neighbors\n",
    "        new_L.append(lazy)\n",
    "    L = new_L                       # swap old list for new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(*L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.compute(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the red bars for inter-worker communication.  Also note how there is lots of parallelism at the beginning but less towards the end as we reach the top of the tree where there is less work to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask array operations\n",
    "Let's take a look at some numpy operations and how to use to dask dashboard to guage the performance.\n",
    "\n",
    "Shut down your cluster and create a new one. Drag the client code across and run it. It will look something like the following:\n",
    "\n",
    "```\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tcp://127.0.0.1:33111\")\n",
    "client\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "x = da.random.random((10000,20000,10), chunks=(1000,1000,5))\n",
    "y = da.random.random((10000,20000,10), chunks=(1000,1000,5))\n",
    "z = (da.arcsin(x) + da.arccos(y)).sum(axis=(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the size and shape of the array and the chunking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call compute and observe the dask task stream, progress and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next example, we observe the effect on performance of a poor choice of chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = da.random.random(10_000_000, chunks=1000) #chunks of size 1000\n",
    "x.sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Note:</b> chunks stands for \"chunk shape\" rather than \"number of chunks\", so specifying chunks=1 means that you will have very many chunks, each with exactly one element!\n",
    "</div>\n",
    "\n",
    "<mark>Question</mark> How does the performance look? \n",
    "\n",
    "<mark>Question</mark> Try to improve things by using larger chunk sizes. \n",
    "\n",
    "<mark>Question</mark> What about only a single chunk. What is this equivalent to, and how does performance compare with that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
